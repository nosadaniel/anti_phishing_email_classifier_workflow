{
  "metrics": {
    "accuracy": 0.636,
    "precision": 0.8874172185430463,
    "recall": 0.44816053511705684,
    "f1": 0.5955555555555555
  },
  "confusion_matrix": {
    "tn": 368,
    "fp": 34,
    "fn": 330,
    "tp": 268
  },
  "results": [
    {
      "index": 0,
      "actual_label": 1,
      "subject": "Never agree to be a loser",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 1,
      "actual_label": 1,
      "subject": "Befriend Jenna Jameson",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 2,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 3,
      "actual_label": 0,
      "subject": "Re: svn commit: r619753 - in /spamassassin/trunk: lib/Mail/SpamAssassin/PerMsgStatus.pm lib/Mail/SpamAssassin/Util/RegistrarBoundaries.pm t/uri_text.t",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 4,
      "actual_label": 1,
      "subject": "SpecialPricesPharmMoreinfo",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 5,
      "actual_label": 1,
      "subject": "From Caroline Aragon",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 6,
      "actual_label": 1,
      "subject": "Replica Watches",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 7,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 8,
      "actual_label": 0,
      "subject": "[Bug 5780] URI processing turns uuencoded strings into http URI's which then causes FPs",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 9,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 10,
      "actual_label": 1,
      "subject": "debt consolidation ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 11,
      "actual_label": 1,
      "subject": "It combines the best of Creative Suite 3 Design Premium, Web Premium, and Production Premium editions",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 12,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 13,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 14,
      "actual_label": 1,
      "subject": "Fifth / Sixth month",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 15,
      "actual_label": 0,
      "subject": "RE: Trial IRC Certificate Application",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 16,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 17,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 18,
      "actual_label": 0,
      "subject": "Re: [opensuse] Why can't I use \"shutdown now\" to turn off my system?",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 19,
      "actual_label": 0,
      "subject": "Re: Fwd: [opensuse] Re: openSUSE Boxed Editions",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 20,
      "actual_label": 0,
      "subject": "ScienceNOW Daily Email Alert",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 21,
      "actual_label": 0,
      "subject": "[perl #46011] overload \"0+\" doesn't handle integer results ",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 22,
      "actual_label": 0,
      "subject": "Wekalist Digest, Vol 60, Issue 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 23,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes storage.py,1.61,1.62",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 24,
      "actual_label": 0,
      "subject": "Re: [opensuse] du -shx issue/question",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 25,
      "actual_label": 1,
      "subject": ";) Look porno Gallery!!!",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 26,
      "actual_label": 1,
      "subject": "Your order",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 27,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 28,
      "actual_label": 1,
      "subject": "Love consultants recommend herbal products",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 29,
      "actual_label": 1,
      "subject": "MBA-Degreees not for sale- get it.",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 30,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 31,
      "actual_label": 1,
      "subject": "Man's stuff store",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 32,
      "actual_label": 1,
      "subject": "Change your weakness to power",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 33,
      "actual_label": 0,
      "subject": "Re: Regex issue with scoping in loops. ",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 34,
      "actual_label": 0,
      "subject": "[Bookscanada] Was a book written about this - - ?",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 35,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 36,
      "actual_label": 1,
      "subject": "Karma sutra and massage foreplay techniques",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 37,
      "actual_label": 1,
      "subject": "Touch her heart with your new babymaker.",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 38,
      "actual_label": 0,
      "subject": "[Bug 5780] [review] URI processing turns uuencoded strings into http URI's which then causes FPs",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 39,
      "actual_label": 1,
      "subject": "Mega-huge dimension for you!",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 40,
      "actual_label": 1,
      "subject": "Tell your  friend how to quit smoking.",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 41,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 42,
      "actual_label": 1,
      "subject": "impeccable impedance facet vincent henbane",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 43,
      "actual_label": 0,
      "subject": "[Crm114-announce] crm114 filters all mails to unsure ..",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 44,
      "actual_label": 0,
      "subject": "WinnipegFreePress.com Breaking News",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 45,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 46,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 47,
      "actual_label": 0,
      "subject": "Weird Stories from NBC11.com",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 48,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 49,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 50,
      "actual_label": 0,
      "subject": "CEAS 2007 Live Spam Challenge",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 51,
      "actual_label": 0,
      "subject": "Re: [opensuse] du -shx issue/question",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 52,
      "actual_label": 0,
      "subject": "Re: [opensuse] The new openSUSE community representative",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 53,
      "actual_label": 0,
      "subject": "[Bug 5813] [review] several TLDs are not parsed by URI text scanner in PerMsgStatus.pm",
      "predicted_label": 0,
      "trust_score": 96,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 54,
      "actual_label": 0,
      "subject": "[Perl Jobs] Perl developer, Hungary, Budapest",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 55,
      "actual_label": 0,
      "subject": "[Perl Jobs] Software Engineer, Weather (onsite), United Kingdom, London",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 56,
      "actual_label": 0,
      "subject": "Organic Bytes: Organic Fraud, Brainwashing Children, Anthrax, Green  Solutions...",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 57,
      "actual_label": 0,
      "subject": "[Perl Jobs] Perl & Apache & Unix/Linux & Mysql/Oracle Software Engineer (part onsite), India, Karnataka, Bangalore",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 58,
      "actual_label": 1,
      "subject": "photo Interesting for 7eac07b0ac7489227e7e5bcf767b2dd3 ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 59,
      "actual_label": 0,
      "subject": "[Perl Jobs] Perl Developer (onsite), United States, CT, Manchester",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 60,
      "actual_label": 0,
      "subject": "[spambayes-dev] WG: false positive",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 61,
      "actual_label": 1,
      "subject": "* All the excitement all the fun and all the money! ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 62,
      "actual_label": 1,
      "subject": "Money back  guarantee to ensure results!",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 63,
      "actual_label": 0,
      "subject": "[R] Mixed model with multiple response variables?",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 64,
      "actual_label": 1,
      "subject": "Make your gf happy",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 65,
      "actual_label": 0,
      "subject": "[Perl Jobs] Open Source Perl Developer  (onsite), United States, California, Sunnyvale",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 66,
      "actual_label": 1,
      "subject": "Mego claybourne suggestion",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 67,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 68,
      "actual_label": 1,
      "subject": "We offer you happiness",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 69,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 70,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 71,
      "actual_label": 1,
      "subject": "ID:19346 The world's largest online prescription-free apothecary",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 72,
      "actual_label": 1,
      "subject": "Ravishing Bvlgari watches at Replica Classics   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 73,
      "actual_label": 0,
      "subject": "[Python-Dev] Python-Dev Summary Draft (April 1-15, 2007)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 74,
      "actual_label": 1,
      "subject": "Splendid Health Joy",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 75,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 76,
      "actual_label": 1,
      "subject": "Succeed in close relationships",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 77,
      "actual_label": 0,
      "subject": "[Perl Jobs] Unix-Scripting Expert (onsite), United States, IL, Schaumburg",
      "predicted_label": 0,
      "trust_score": 85,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 78,
      "actual_label": 1,
      "subject": "Perfected Health Joy",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 79,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 80,
      "actual_label": 1,
      "subject": "Premo Narcotic Discounts",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 81,
      "actual_label": 1,
      "subject": "Splendid Rx Joy",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 82,
      "actual_label": 1,
      "subject": "Chanel Watches",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 83,
      "actual_label": 0,
      "subject": "Re: Change 32008: Upgrade to CPAN 1.9203",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 84,
      "actual_label": 1,
      "subject": "Katerina age 29 -on dating ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 85,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 86,
      "actual_label": 1,
      "subject": "Satisfy your woman?s craving easily",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 87,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 88,
      "actual_label": 0,
      "subject": "Supervisors PhD students: 6-monthly reports...",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 89,
      "actual_label": 1,
      "subject": "Great pharm shop",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 90,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 91,
      "actual_label": 0,
      "subject": "[perl #45981] socket memory leak + test case ",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 92,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] Python-Dev Summary Draft (April 1-15, 2007)",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 93,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] Python-Dev Summary Draft (April 1-15, 2007)",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 94,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 95,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] Python-Dev Summary Draft (April 1-15, 2007)",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 96,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] Python-Dev Summary Draft (April 1-15, 2007)",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 97,
      "actual_label": 0,
      "subject": "[Perl Jobs] Top NYC LAMP shop / B2B mod_perl / Perl developer (onsite), United States, NY, New York",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 98,
      "actual_label": 0,
      "subject": "[SM-USERS] SM 1.4.13 Configuration Question: Mail Domain Parameter",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 99,
      "actual_label": 0,
      "subject": "ACNielsen Your Voice * 150 e-points for survey completion O101300116\t(ONL3038) B",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 100,
      "actual_label": 1,
      "subject": "CialisEffectivenessAllProducts",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 101,
      "actual_label": 0,
      "subject": "Wekalist Digest, Vol 60, Issue 11",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 102,
      "actual_label": 1,
      "subject": "Angelina Jolie Free Video.",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 103,
      "actual_label": 1,
      "subject": "us debt",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 104,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 105,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 106,
      "actual_label": 1,
      "subject": "The search for high quality m_edz and save your money?",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 107,
      "actual_label": 1,
      "subject": "Items for Man's health",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 108,
      "actual_label": 1,
      "subject": "world of clocks",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 109,
      "actual_label": 0,
      "subject": "Emerging Technologies Friday Update (02/08/2008)",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 110,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 111,
      "actual_label": 1,
      "subject": "World largest selection of medical products for men and women.",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 112,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 113,
      "actual_label": 1,
      "subject": "Help Stop Premature Ejaculation! ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 114,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 115,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] Python-Dev Summary Draft (April 1-15, 2007)",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 116,
      "actual_label": 1,
      "subject": "123",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 117,
      "actual_label": 1,
      "subject": "Immense dimension of your monster",
      "predicted_label": 0,
      "trust_score": 40,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 118,
      "actual_label": 0,
      "subject": "[UAI] [3dpvt2004] 3DPVT 2004: Technical Program and Registration",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 119,
      "actual_label": 0,
      "subject": "sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 120,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 121,
      "actual_label": 0,
      "subject": "Re: [doap-interest] New DOAP repository: doapspace.org",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 122,
      "actual_label": 0,
      "subject": "Re: svn commit: r619753 - in /spamassassin/trunk: lib/Mail/SpamAssassin/PerMsgStatus.pm lib/Mail/SpamAssassin/Util/RegistrarBoundaries.pm t/uri_text.t",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 123,
      "actual_label": 0,
      "subject": "Re: Regex issue with scoping in loops.",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 124,
      "actual_label": 0,
      "subject": "[spambayes-dev] [patches] IMAP and train-to-exhaustion",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 125,
      "actual_label": 0,
      "subject": "PhD 6 monthly reports...thanks...",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 126,
      "actual_label": 0,
      "subject": "Valentine, be on time! Shop by Sunday",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 127,
      "actual_label": 0,
      "subject": "Re: [opensuse] du -shx issue/question",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 128,
      "actual_label": 1,
      "subject": "Good day !",
      "predicted_label": 1,
      "trust_score": 10,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 129,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 130,
      "actual_label": 1,
      "subject": "Love doctors recommend flirting as cure",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 131,
      "actual_label": 0,
      "subject": "Science Scienceexpress Notification for 7 Feb 2008",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 132,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 133,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 134,
      "actual_label": 1,
      "subject": "Interesting Medical Alternatives",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 135,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 136,
      "actual_label": 0,
      "subject": "Science CiteTrack: Editors' Choice: Highlights of the recent literature",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 137,
      "actual_label": 1,
      "subject": "Do European Union Drugs not refuse him in plesure",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 138,
      "actual_label": 1,
      "subject": "Massage techniques to help your man pole",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 139,
      "actual_label": 1,
      "subject": "SOLD OUT -- -Looking for perfect gift? Buy Rolex    4vwd",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 140,
      "actual_label": 1,
      "subject": "SOLD OUT -- -Looking for perfect gift? Buy Rolex    4vwd",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 141,
      "actual_label": 1,
      "subject": "Which Ones Really Work? We List The Top Penis Enlargement Products!",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 142,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 143,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5734)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 144,
      "actual_label": 0,
      "subject": "5 new messages in 4 topics - abridged",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 145,
      "actual_label": 0,
      "subject": "29 new messages in 14 topics - abridged",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 146,
      "actual_label": 0,
      "subject": "Re: flooded with jr* spam",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 147,
      "actual_label": 0,
      "subject": "[Bug 5813] [review] several TLDs are not parsed by URI text scanner in PerMsgStatus.pm",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 148,
      "actual_label": 0,
      "subject": "Re: permission denied while using pipe in master.cf",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 149,
      "actual_label": 0,
      "subject": "Re: [spambayes-dev] [patches] IMAP and train-to-exhaustion",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 150,
      "actual_label": 0,
      "subject": "[TechRepublic] Geekend: Sci-fi books that make you dumb, C64 lives",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 151,
      "actual_label": 1,
      "subject": "Your order",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 152,
      "actual_label": 0,
      "subject": "[ spambayes-Patches-1707814 ] More resilient parsing for sb_imapfilter",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 153,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5735)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 154,
      "actual_label": 1,
      "subject": "It will rise faster and stay up longer.",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 155,
      "actual_label": 1,
      "subject": "Join the only real club! The winner's club! Winning is about money and US players know $2400 when they see them! Seeing is winning at our casino. Walk away a winner just for visiting.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 156,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 157,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 158,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 159,
      "actual_label": 1,
      "subject": "For: 90e5bd77f1c295c6230bf2386176ab32 Stock Markets Close As Global Earth World Planet International Buys All Shares",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 160,
      "actual_label": 1,
      "subject": "From Brandie Stinson",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 161,
      "actual_label": 0,
      "subject": "[Bug 5813] [review] several TLDs are not parsed by URI text scanner in PerMsgStatus.pm",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 162,
      "actual_label": 0,
      "subject": "Re: [spambayes-dev] [patches] IMAP and train-to-exhaustion",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 163,
      "actual_label": 0,
      "subject": "caller() info in DESTROY off?",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 164,
      "actual_label": 0,
      "subject": "Re: [doap-interest] New DOAP repository: doapspace.org",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 165,
      "actual_label": 1,
      "subject": "$500 dollars without deposit!",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 166,
      "actual_label": 0,
      "subject": "[ spambayes-Bugs-1707852 ] Can't classify words beginning with \"tem\"",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 167,
      "actual_label": 0,
      "subject": "Science CiteTrack: This Week In Science",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 168,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5736)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 169,
      "actual_label": 0,
      "subject": "Re: caller() info in DESTROY off?",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 170,
      "actual_label": 1,
      "subject": "Your order is executed",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 171,
      "actual_label": 0,
      "subject": "Friends & Family - 15% OFF your entire order - spread the word!",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 172,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 173,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 174,
      "actual_label": 1,
      "subject": "You can increase the time of your sexual act ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 175,
      "actual_label": 1,
      "subject": "Help Stop Premature Ejaculation! ",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 176,
      "actual_label": 1,
      "subject": "ALL MAJOR DESIGNER REPLICA W ATCHES!   ",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 177,
      "actual_label": 1,
      "subject": "You Can Enlarge Your Penis By 3\"",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 178,
      "actual_label": 1,
      "subject": "Katerina age 29 -on dating ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 179,
      "actual_label": 1,
      "subject": "Exclusive stuff for mans",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 180,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5737)",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 181,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 182,
      "actual_label": 1,
      "subject": "How to make money like a porn star",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 183,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5738)",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 184,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 185,
      "actual_label": 0,
      "subject": "Re: svn commit: r619753 - in /spamassassin/trunk: lib/Mail/SpamAssassin/PerMsgStatus.pm lib/Mail/SpamAssassin/Util/RegistrarBoundaries.pm t/uri_text.t ",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 186,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 187,
      "actual_label": 0,
      "subject": "[Bug 5790] update INSTALL docs to note DNSBL usage bug in Solaris 8",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 188,
      "actual_label": 0,
      "subject": "[Bug 5790] update INSTALL docs to note DNSBL usage bug in Solaris 8",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 189,
      "actual_label": 0,
      "subject": "postfix + dkim-filter + email lists - milter troubles (can't read SMFIC_HEADER",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 190,
      "actual_label": 0,
      "subject": "Orvis New York News",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 191,
      "actual_label": 0,
      "subject": "[duodenalswitch] Digest Number 13712",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 192,
      "actual_label": 1,
      "subject": "You don't have to worry about paying exorbitant prices  for your medications anymore.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 193,
      "actual_label": 1,
      "subject": "Replica Watches",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 194,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5739)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 195,
      "actual_label": 1,
      "subject": "Attract more hot women",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 196,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 197,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 198,
      "actual_label": 0,
      "subject": "[perl #46019] Documentation for 'srand' has a problem in code snippet ",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 199,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 200,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 201,
      "actual_label": 1,
      "subject": "Jenna Jameson wants you as BFF",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 202,
      "actual_label": 0,
      "subject": "Mail Sending problem",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 203,
      "actual_label": 1,
      "subject": "Impress others with your style",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 204,
      "actual_label": 1,
      "subject": "Jenna Jameson wants you as BFF",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 205,
      "actual_label": 1,
      "subject": "This works for any product, website or affiliate website!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 206,
      "actual_label": 1,
      "subject": "Exclusive models of HQ rep1!c@s",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 207,
      "actual_label": 0,
      "subject": "Call for Papers - NZCSRSC 08",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 208,
      "actual_label": 0,
      "subject": "Dice JobAlert for Agent Primary",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 209,
      "actual_label": 0,
      "subject": "[Python-Dev] April 1-15, 2007 Summaries Final Draft",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 210,
      "actual_label": 0,
      "subject": "[Bug 5813] [review] several TLDs are not parsed by URI text scanner in PerMsgStatus.pm",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 211,
      "actual_label": 0,
      "subject": "clamav-users Digest, Vol 41, Issue 6",
      "predicted_label": 0,
      "trust_score": 90,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 212,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5740)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 213,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] squirrelmail v1.4.13 and Login Redirect Plugin v2.1",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 214,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 215,
      "actual_label": 0,
      "subject": "Science Table of Contents Text for Cities: 8 February 2008; Vol. 319, No. 5864",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 216,
      "actual_label": 1,
      "subject": "Drink from the fountain of youth",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 217,
      "actual_label": 0,
      "subject": "TuxOnIce-users Digest, Vol 36, Issue 10",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 218,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 219,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 220,
      "actual_label": 0,
      "subject": "[UAI] ECIR'05: Call for papers",
      "predicted_label": 0,
      "trust_score": 90,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 221,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 222,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 223,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 224,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 225,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 226,
      "actual_label": 0,
      "subject": "[Bug 5815]  New: New TLD .rs",
      "predicted_label": 0,
      "trust_score": 1,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 227,
      "actual_label": 1,
      "subject": "Recommended plastic surgery for men",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 228,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 229,
      "actual_label": 1,
      "subject": "Amazingly fast payouts",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 230,
      "actual_label": 0,
      "subject": "[Python-Dev] os.rename on windows",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 231,
      "actual_label": 1,
      "subject": "Aim for great success",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 232,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 233,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 234,
      "actual_label": 0,
      "subject": "SASL questions",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 235,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 236,
      "actual_label": 0,
      "subject": "Re: [dkim-milter-discuss] postfix + dkim-filter + email lists - milter  troubles (can't read SMFIC_HEADER",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 237,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 238,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5741)",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 239,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 240,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 241,
      "actual_label": 0,
      "subject": "Google Alert - \"active learning\"",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 242,
      "actual_label": 0,
      "subject": "LAST DAY for Free Express Upgrade for Valentine's Day!",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 243,
      "actual_label": 0,
      "subject": "Re: [dkim-milter-discuss] postfix + dkim-filter + email lists - milter troubles (can't read SMFIC_HEADER",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 244,
      "actual_label": 0,
      "subject": "Service alert: spamassassin.zones.apache.org/BBMASS is WARNING",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 245,
      "actual_label": 0,
      "subject": "Re: [R] Greek characters in plots",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 246,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 247,
      "actual_label": 0,
      "subject": "Re: [perl #46019] Documentation for 'srand' has a problem in code snippet",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 248,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 249,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 250,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 251,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5742)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 252,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 253,
      "actual_label": 1,
      "subject": "Lose 10 pounds in 3 weeks",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 254,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 255,
      "actual_label": 0,
      "subject": "Service alert: spamassassin.zones.apache.org/BBMASS is CRITICAL",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 256,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 257,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 258,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 259,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 260,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 261,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 262,
      "actual_label": 1,
      "subject": "Gain bonus inches without side effects",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 263,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 264,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 265,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 266,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 267,
      "actual_label": 1,
      "subject": "Gain bonus inches without side effects",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 268,
      "actual_label": 0,
      "subject": "Service alert: spamassassin.zones.apache.org/BBMASS is WARNING",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 269,
      "actual_label": 0,
      "subject": "[ie-rant] hp pavilion issues",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 270,
      "actual_label": 0,
      "subject": "Is mobile really a sure thing for Google? | CNET News.com Alert ",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 271,
      "actual_label": 0,
      "subject": "Will IM ever kick off its shackles? | CNET News.com Alert ",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 272,
      "actual_label": 0,
      "subject": "Sandisk 4GB Titanium $19.99 . 2GB 150x SD $4 . 4GB MicroSD $19",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 273,
      "actual_label": 0,
      "subject": "Service alert: spamassassin.zones.apache.org/BBMASS is OK",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 274,
      "actual_label": 1,
      "subject": "Become more strong and mighty",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 275,
      "actual_label": 1,
      "subject": "Become more strong and mighty",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 276,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 277,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 278,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 279,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 280,
      "actual_label": 1,
      "subject": "Unique men's health shop",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 281,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 282,
      "actual_label": 1,
      "subject": "From Sonja Henley",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 283,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 284,
      "actual_label": 0,
      "subject": "[SM-USERS] errors",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 285,
      "actual_label": 1,
      "subject": "Karma sutra and massage foreplay techniques",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 286,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 287,
      "actual_label": 1,
      "subject": "Man fined for having long penn",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 288,
      "actual_label": 0,
      "subject": "Re: sa-learn weirdness...",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 289,
      "actual_label": 1,
      "subject": "video with a naked celebrity Kick-up for b71af2efa87d808060a15b1f88394556 ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 290,
      "actual_label": 1,
      "subject": "Womens wish list for men",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 291,
      "actual_label": 1,
      "subject": "There's a shop for beauty products, pharmacy called Canadian Chemists.",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 292,
      "actual_label": 1,
      "subject": "Satisfy your partner every night",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 293,
      "actual_label": 0,
      "subject": "3 Day Weekend Sale - Low Prices This Weekend Only!",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 294,
      "actual_label": 0,
      "subject": "Re: [dkim-milter-discuss] postfix + dkim-filter + email lists - milter troubles (can't read SMFIC_HEADER",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 295,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 296,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 297,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 298,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5744)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 299,
      "actual_label": 0,
      "subject": "Re: [dkim-milter-discuss] postfix + dkim-filter + email lists - milter troubles (can't read SMFIC_HEADER",
      "predicted_label": 1,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 300,
      "actual_label": 0,
      "subject": "PhD 6-monthly reports...",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 301,
      "actual_label": 0,
      "subject": "Wekalist Digest, Vol 60, Issue 12",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 302,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 303,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 304,
      "actual_label": 0,
      "subject": "Re: Virtual Config Dir Problem",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 305,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 306,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 307,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 308,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] os.rename on windows",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 309,
      "actual_label": 1,
      "subject": "Your Digital Greeting Card is waiting",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 310,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 311,
      "actual_label": 1,
      "subject": "Force men things",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 312,
      "actual_label": 0,
      "subject": "[UAI] WIRN04 Technical program",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 313,
      "actual_label": 1,
      "subject": "Highly secure 256bit order processing",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 314,
      "actual_label": 1,
      "subject": "Money back  guarantee to ensure results!",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 315,
      "actual_label": 1,
      "subject": "High Quality Rolex Replica Watches",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 316,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 317,
      "actual_label": 1,
      "subject": "Love doctors recommend flirting as cure",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 318,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 319,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 320,
      "actual_label": 1,
      "subject": "*3 FREE Bottles Of VPXL !! ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 321,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 322,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 323,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 324,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 325,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 326,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] os.rename on windows",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 327,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 328,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 329,
      "actual_label": 0,
      "subject": "You're Invited to Our Friends & Family Event",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 330,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] os.rename on windows",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 331,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 332,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 333,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 334,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 335,
      "actual_label": 0,
      "subject": "Re: [dkim-milter-discuss] postfix + dkim-filter + email lists - milter  troubles (can't read SMFIC_HEADER",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 336,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 337,
      "actual_label": 0,
      "subject": "[Bug 5790] update INSTALL docs to note DNSBL usage bug in Solaris 8",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 338,
      "actual_label": 1,
      "subject": "Proven Effective for 72 Hours Free Trial Sample Available Today",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 339,
      "actual_label": 1,
      "subject": "Dont be left behind because of bad alth.",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 340,
      "actual_label": 1,
      "subject": "Give freedom of your dreams",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 341,
      "actual_label": 0,
      "subject": "[Bug 5816]  New: AWL AutoWhiteList",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 342,
      "actual_label": 1,
      "subject": "Qualitative replica watches for most exacting people   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 343,
      "actual_label": 1,
      "subject": "Keep the young girls away from that bulch",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 344,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 345,
      "actual_label": 1,
      "subject": "Quit smoking  easily with anti-smoking patches.",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 346,
      "actual_label": 1,
      "subject": "Love doctors recommend flirting as cure",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 347,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] os.rename on windows",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 348,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 349,
      "actual_label": 1,
      "subject": "Perfect proportions are easily attained",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 350,
      "actual_label": 1,
      "subject": "fw: \u041e\u043a\u043d\u0430 \u041f\u0412\u0425-\u0417\u0410\u0411\u0423\u0414\u042c\u0422\u0415 \u041f\u0420\u041e \u0428\u0423\u041c!",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 351,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 352,
      "actual_label": 1,
      "subject": "Choose the best online drugstore !",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 353,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 354,
      "actual_label": 0,
      "subject": "Re: Mail Sending problem",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 355,
      "actual_label": 1,
      "subject": "100% Money Back Guarantee. ",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 356,
      "actual_label": 0,
      "subject": "buildbot failure in mleisi-suse10.2-x86",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 357,
      "actual_label": 1,
      "subject": "Answers to your love relationship",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 358,
      "actual_label": 0,
      "subject": "BCC mail 'storms'",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 359,
      "actual_label": 0,
      "subject": "Get the best that TechRepublic has to offer in a single download! | ZDNet Announcements",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 360,
      "actual_label": 0,
      "subject": "[Bug 5814] uri PORN_4 regex false positive on http://www.google-analytics.com",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 361,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 362,
      "actual_label": 0,
      "subject": "Find a Sweet Deal This Valentine's Day at GNC!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 363,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 364,
      "actual_label": 0,
      "subject": "Webcast Roundup: Exchange 2007 deployment case study, mailbox role memory characterizations, and more",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 365,
      "actual_label": 0,
      "subject": "[spambayes-dev] I used SpamBayes for class project - thanks!",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 366,
      "actual_label": 0,
      "subject": "Re: sa-learn weirdness...",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 367,
      "actual_label": 1,
      "subject": "Open now for your eCard",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 368,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 369,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 370,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 371,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 372,
      "actual_label": 1,
      "subject": "Unforgettable views",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 373,
      "actual_label": 0,
      "subject": "Science CiteTrack: Science News This Week",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 374,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 375,
      "actual_label": 1,
      "subject": "Karma sutra and massage foreplay techniques",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 376,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 377,
      "actual_label": 0,
      "subject": "Webcast Roundup: Adobe Acrobat 8: Solving Today's Communication Challenges and more",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 378,
      "actual_label": 1,
      "subject": "do you have 10 inches? Maybe You want enlarge him",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 379,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 380,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 381,
      "actual_label": 1,
      "subject": "How to make money like a porn star",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 382,
      "actual_label": 0,
      "subject": "New White Paper: Terminal emulation: Overview and best practices",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 383,
      "actual_label": 1,
      "subject": "Satisfy your partner every night",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 384,
      "actual_label": 1,
      "subject": "Karma sutra and massage foreplay techniques",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 385,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 386,
      "actual_label": 1,
      "subject": "Get hung like no one else",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 387,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 388,
      "actual_label": 0,
      "subject": "Re: BCC mail 'storms'",
      "predicted_label": 0,
      "trust_score": 1,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 389,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 390,
      "actual_label": 1,
      "subject": "Goodiest giovanne solution",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 391,
      "actual_label": 0,
      "subject": "New White Paper: Deploy New Technology and Applications Without Downtime or Disruption",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 392,
      "actual_label": 0,
      "subject": "[spambayes-dev] Vista compatibilty",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 393,
      "actual_label": 0,
      "subject": "Re: Mail Sending problem",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 394,
      "actual_label": 0,
      "subject": "Re: [ie-rant] back-up incremental software",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 395,
      "actual_label": 0,
      "subject": "Webcast & Podcast Roundup: Best Practices for Infrastructure Alignment of the Remote Office and more",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 396,
      "actual_label": 0,
      "subject": "Re: Mail Sending problem",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 397,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 398,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 399,
      "actual_label": 0,
      "subject": "Re: [perl #45513] Test failures on amd64-freebsd 6.2",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 400,
      "actual_label": 0,
      "subject": "First Steps to Unified Communications and More",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 401,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 402,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 403,
      "actual_label": 1,
      "subject": "Man's stuff mall",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 404,
      "actual_label": 0,
      "subject": "fedora-announce-list Digest, Vol 48, Issue 4",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 405,
      "actual_label": 0,
      "subject": "Mail attachements truncated",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 406,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 407,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 408,
      "actual_label": 1,
      "subject": "Be not afraid of making changes in your life",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 409,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 410,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 411,
      "actual_label": 0,
      "subject": "[Bug 5816] AWL AutoWhiteList",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 412,
      "actual_label": 1,
      "subject": "How to make money like a porn star",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 413,
      "actual_label": 0,
      "subject": "Science Table of Contents Posting Notification for Cities: 8 February 2008; Vol. 319, No. 5864",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 414,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 415,
      "actual_label": 0,
      "subject": "Re: [patch@31780] fixes for cpanplus on VMS",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 416,
      "actual_label": 1,
      "subject": "Recommended plastic surgery for men",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 417,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 418,
      "actual_label": 1,
      "subject": "From Casandra Moss",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 419,
      "actual_label": 0,
      "subject": "Re: Mail attachements truncated",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 420,
      "actual_label": 1,
      "subject": "CanadianFastShippingGenerics",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 421,
      "actual_label": 1,
      "subject": "Jenna Jameson wants you as BFF",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 422,
      "actual_label": 0,
      "subject": "Feb. 14th: Top trends impacting two-factor authentication",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 423,
      "actual_label": 1,
      "subject": "Jenna Jameson wants you as BFF",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 424,
      "actual_label": 1,
      "subject": "Classy things you could give to your folks",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 425,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/contrib tte.py,1.17,1.18",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 426,
      "actual_label": 1,
      "subject": "Jenna Jameson wants you as BFF",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 427,
      "actual_label": 0,
      "subject": "[UAI] CfP: DATE05: Subtrack Real-time Systems",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 428,
      "actual_label": 0,
      "subject": "[SM-DEVEL] SubFolders untranslated",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 429,
      "actual_label": 0,
      "subject": "[Bug 5816] AWL AutoWhiteList",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 430,
      "actual_label": 0,
      "subject": "Re: Getopt::Long and Log::StdLog problem",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 431,
      "actual_label": 1,
      "subject": "SoftTabsFDAapprovedNewProducts",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 432,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 433,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes mboxutils.py, 1.10,\t1.11 storage.py, 1.62, 1.63",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 434,
      "actual_label": 0,
      "subject": "Prizes and Awards: It's That Time of Year",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 435,
      "actual_label": 0,
      "subject": "Site-Wide Savings!",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 436,
      "actual_label": 0,
      "subject": "[Bug 5816] AWL AutoWhiteList",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 437,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 438,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 439,
      "actual_label": 0,
      "subject": "Podcast update: Threats and trends for 2008",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 440,
      "actual_label": 1,
      "subject": "Answers to your love relationship",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 441,
      "actual_label": 1,
      "subject": "Man fined for having long penn",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 442,
      "actual_label": 0,
      "subject": "[Bug 5814] uri PORN_4 regex false positive on http://www.google-analytics.com",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 443,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 444,
      "actual_label": 1,
      "subject": "Love doctors recommend flirting as cure",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 445,
      "actual_label": 1,
      "subject": "Buy  your medications at the best possible prices.",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 446,
      "actual_label": 0,
      "subject": "Customer Appreciation Savings ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 447,
      "actual_label": 1,
      "subject": "What They Don't Want You to Know What it Does to Your Body !",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 448,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 449,
      "actual_label": 1,
      "subject": "machine impedance society society monash",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 450,
      "actual_label": 1,
      "subject": "Drink from the fountain of youth",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 451,
      "actual_label": 1,
      "subject": "Tell your  friend how to quit smoking.",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 452,
      "actual_label": 1,
      "subject": "How To Enlarge Penis Size",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 453,
      "actual_label": 1,
      "subject": "Get a magic wand to satisfy your lady",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 454,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 455,
      "actual_label": 1,
      "subject": "Drink from the fountain of youth",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 456,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 457,
      "actual_label": 1,
      "subject": "Thanks for  reading this information!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 458,
      "actual_label": 1,
      "subject": "Befriend Jenna Jameson",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 459,
      "actual_label": 0,
      "subject": "Analysts to Microsoft: Save XP!",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 460,
      "actual_label": 1,
      "subject": "Let other people  know about your successful quitting.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 461,
      "actual_label": 1,
      "subject": "Penis Enlargment Reviews",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 462,
      "actual_label": 1,
      "subject": "Big Dicks Movies",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 463,
      "actual_label": 1,
      "subject": "Extend your possibilities in your private life",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 464,
      "actual_label": 0,
      "subject": "[SM-USERS]  Autorespond plugin",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 465,
      "actual_label": 1,
      "subject": "High Quality Rolex Replica Watches",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 466,
      "actual_label": 1,
      "subject": "Offer variety replica watches",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 467,
      "actual_label": 1,
      "subject": "From Rickey Case",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 468,
      "actual_label": 1,
      "subject": "Womens wish list for men",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 469,
      "actual_label": 0,
      "subject": "Forwarding all localmail ",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 470,
      "actual_label": 1,
      "subject": "Your order is executed",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 471,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 472,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 473,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 474,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 475,
      "actual_label": 1,
      "subject": "Good jeramey proposition",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 476,
      "actual_label": 1,
      "subject": "How to make money like a porn star",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 477,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 478,
      "actual_label": 1,
      "subject": "Huge dimension gives increased force",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 479,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/scripts sb_imapfilter.py,1.66,1.67",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 480,
      "actual_label": 1,
      "subject": "Compressed happiness",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 481,
      "actual_label": 1,
      "subject": "fashionable replica watches",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 482,
      "actual_label": 0,
      "subject": "Beta testers have had Vista for two weeks",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 483,
      "actual_label": 0,
      "subject": "Trade Me -- Please place feedback",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 484,
      "actual_label": 0,
      "subject": "Re: Forwarding all localmail",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 485,
      "actual_label": 0,
      "subject": "Re: Forwarding all localmail",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 486,
      "actual_label": 0,
      "subject": "Google Alert - label unlabeled data",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 487,
      "actual_label": 1,
      "subject": "Achieve any length you need",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 488,
      "actual_label": 0,
      "subject": "[UAI] Three Faculty Positions in CS at University College London",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 489,
      "actual_label": 0,
      "subject": "[UAI] parameter estimation question",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 490,
      "actual_label": 0,
      "subject": "svn commit: r581572 - /spamassassin/rules/trunk/sandbox/jm/20_basic.cf",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 491,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 492,
      "actual_label": 1,
      "subject": "Katerina age 29 -on dating ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 493,
      "actual_label": 1,
      "subject": "Superior Narcotic Satisfactions",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 494,
      "actual_label": 1,
      "subject": "Interesting Herbal Deals",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 495,
      "actual_label": 1,
      "subject": "Befriend Jenna Jameson",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 496,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 497,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 498,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 499,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] Bug Report",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 500,
      "actual_label": 1,
      "subject": "rolex cartier omega LV CD chanel gucci ...  ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 501,
      "actual_label": 1,
      "subject": "Love consultants recommend herbal products",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 502,
      "actual_label": 0,
      "subject": "[ spambayes-Patches-1707808 ] imap-tte",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 503,
      "actual_label": 1,
      "subject": "Bring more happiness into your holiday mood ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 504,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 505,
      "actual_label": 0,
      "subject": "Windows Vista, round two",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 506,
      "actual_label": 0,
      "subject": "New White Paper: Why Virtual Servers Need Virtual Storage",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 507,
      "actual_label": 1,
      "subject": "100% Money Back Guarantee. ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 508,
      "actual_label": 0,
      "subject": "Re: Forwarding all localmail",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 509,
      "actual_label": 0,
      "subject": "Re: score",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 510,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] Bug Report",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 511,
      "actual_label": 0,
      "subject": "Re: score",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 512,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/contrib tte.py,1.18,1.19",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 513,
      "actual_label": 0,
      "subject": "Re: score",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 514,
      "actual_label": 0,
      "subject": "Update of LC Science Tracer Bullet: Wind Power",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 515,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 516,
      "actual_label": 1,
      "subject": "Enlarge",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 517,
      "actual_label": 1,
      "subject": "Super abraham offer",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 518,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes hammie.py,1.16,1.17",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 519,
      "actual_label": 0,
      "subject": "Re: Forwarding all localmail ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 520,
      "actual_label": 1,
      "subject": "Prodigious preparation is to your service",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 521,
      "actual_label": 1,
      "subject": "A Digital Card from someone who cares.",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 522,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 523,
      "actual_label": 1,
      "subject": "Offer variety replica watches",
      "predicted_label": 0,
      "trust_score": 50,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 524,
      "actual_label": 0,
      "subject": "Re: [patch] 00_CPANPLUS-Internals-Utils.t version match fix",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 525,
      "actual_label": 0,
      "subject": "[Dixielandjazz] Fw: about your music..",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 526,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 527,
      "actual_label": 1,
      "subject": "Fulfill  all your pharmaceutical needs with Canadian Pharmacy.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 528,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 529,
      "actual_label": 1,
      "subject": "Love doctors recommend flirting as cure",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 530,
      "actual_label": 0,
      "subject": "RE: Logical Names on VMS (was: [patch@31780] fixes for cpanplus on VMS)",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 531,
      "actual_label": 1,
      "subject": "our sexual life maybe incredible and exciting",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 532,
      "actual_label": 1,
      "subject": "Choice IWC replica watches at Replica Classics   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 533,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 534,
      "actual_label": 0,
      "subject": "Quiz: What the heck are these people talking about?",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 535,
      "actual_label": 0,
      "subject": "perl5.005_05-MAINT20070902",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 536,
      "actual_label": 1,
      "subject": "Man fined for having long penn",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 537,
      "actual_label": 1,
      "subject": "Massage techniques to help your man pole",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 538,
      "actual_label": 1,
      "subject": "For:cbf21ce47b9b84801ce8799e6e7b363c Britney and Paris having fun together. Watch.",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 539,
      "actual_label": 1,
      "subject": "Katerina age 29 -on dating ",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 540,
      "actual_label": 0,
      "subject": "Re: [patch@31780] CPANPLUS/Internals/Constants.pm for VMS",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 541,
      "actual_label": 0,
      "subject": "Re: [spambayes-dev] Standalone SpamBayes classifier for websites",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 542,
      "actual_label": 0,
      "subject": "ietf-dkim Digest, Vol 106, Issue 5",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 543,
      "actual_label": 0,
      "subject": "Re: A question on rulesrc/sandbox/",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 544,
      "actual_label": 0,
      "subject": "[Bug 4104] Several useful URI rules",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 545,
      "actual_label": 0,
      "subject": "[UAI] KI2004: 2nd Call For Participation",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 546,
      "actual_label": 0,
      "subject": "Carrot2-developers Digest, Vol 21, Issue 3",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 547,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 548,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CoreUI.py, NONE,\t1.1.2.1 Options.py, 1.141, 1.141.2.1",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 549,
      "actual_label": 1,
      "subject": "For:8b041062bcf7766253bcd0284e2df718 Paris and Britney nude.",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 550,
      "actual_label": 1,
      "subject": "Drink from the fountain of youth",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 551,
      "actual_label": 0,
      "subject": "Re: Mail attachements truncated",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 552,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 553,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 554,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 555,
      "actual_label": 1,
      "subject": "Perfected Substance Offers",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 556,
      "actual_label": 0,
      "subject": "[Bug 5790] update INSTALL docs to note DNSBL usage bug in Solaris 8",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 557,
      "actual_label": 1,
      "subject": "Greetings from...?",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 558,
      "actual_label": 0,
      "subject": "McCain",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 559,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 560,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 561,
      "actual_label": 1,
      "subject": "Replica Classics trendy replica watches for you   ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 562,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/scripts core_server.py, NONE, 1.1.2.1",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 563,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 564,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 565,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 566,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes setup.py,1.32,1.32.2.1",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 567,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 568,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 569,
      "actual_label": 1,
      "subject": "candace waterway lignum network complainant",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 570,
      "actual_label": 0,
      "subject": "Re: A few multiple dispatch questions",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 571,
      "actual_label": 0,
      "subject": "[UAI] cfp: FLAIRS-2005",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 572,
      "actual_label": 1,
      "subject": "Penis Enlargement Is Possible ! Learn How and What Products Work",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 573,
      "actual_label": 1,
      "subject": "The art of timekeeping",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 574,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 575,
      "actual_label": 1,
      "subject": "High Quality Rolex Replica Watches",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 576,
      "actual_label": 0,
      "subject": "TREC 2007 Spam Track",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 577,
      "actual_label": 0,
      "subject": "Ensuring Code Quality in Multi-threaded Applications",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 578,
      "actual_label": 0,
      "subject": "[Bug 5813] [review] several TLDs are not parsed by URI text scanner in PerMsgStatus.pm",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 579,
      "actual_label": 0,
      "subject": "Off the Shelf - February 8, 2008",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 580,
      "actual_label": 0,
      "subject": "Re: TREC 2007 Spam Track",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 581,
      "actual_label": 0,
      "subject": "Re: Mail attachements truncated",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 582,
      "actual_label": 0,
      "subject": "[Bug 5667] add new URIBL lookup on rhsbl.ahbl.org",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 583,
      "actual_label": 0,
      "subject": "[Perl Jobs] Software Engineer (onsite), United States, CA, San Mateo",
      "predicted_label": 0,
      "trust_score": 90,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 584,
      "actual_label": 1,
      "subject": "Male Enhancement",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 585,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CoreUI.py, 1.1.2.1, 1.1.2.2",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 586,
      "actual_label": 1,
      "subject": "123",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 587,
      "actual_label": 1,
      "subject": "Jaeger-LeCoultre replica watch Luxury isnt a sin ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 588,
      "actual_label": 1,
      "subject": "SuIT COULD BE YOU..",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 589,
      "actual_label": 0,
      "subject": "svn commit: r581580 - /spamassassin/rules/trunk/sandbox/jm/20_bug4104_uribl.cf",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 590,
      "actual_label": 0,
      "subject": "Re: Mail attachements truncated",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 591,
      "actual_label": 1,
      "subject": "Ravishing Bvlgari watches at Replica Classics   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 592,
      "actual_label": 1,
      "subject": "Looking for a watch? Visit Replica Classics ",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 593,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 594,
      "actual_label": 1,
      "subject": "Revolutionary mens health aid has arrived",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 595,
      "actual_label": 0,
      "subject": "server is rejecting mail from single domain",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 596,
      "actual_label": 1,
      "subject": "Karma sutra and massage foreplay techniques",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 597,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 598,
      "actual_label": 1,
      "subject": "123",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 599,
      "actual_label": 1,
      "subject": "Drink from the fountain of youth",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 600,
      "actual_label": 1,
      "subject": "Katerina age 29 -on dating ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 601,
      "actual_label": 0,
      "subject": "Re: permission denied while using pipe in master.cf",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 602,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 603,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 604,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 605,
      "actual_label": 1,
      "subject": "Payment confirmation #1047993667197627481",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 606,
      "actual_label": 1,
      "subject": "Karma sutra and massage foreplay techniques",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 607,
      "actual_label": 0,
      "subject": "Re: server is rejecting mail from single domain",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 608,
      "actual_label": 1,
      "subject": "Love doctors recommend flirting as cure",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 609,
      "actual_label": 1,
      "subject": "Your order",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 610,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 611,
      "actual_label": 0,
      "subject": "Re: Forwarding all localmail",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 612,
      "actual_label": 0,
      "subject": "<FRIDAY>  Networked Storage, SOX Compliance and Demand Management",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 613,
      "actual_label": 0,
      "subject": "Re: server is rejecting mail from single domain",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 614,
      "actual_label": 0,
      "subject": "[ spambayes-Bugs-1722848 ] sb_imapfilter.py failure",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 615,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes storage.py,1.63,1.63.2.1",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 616,
      "actual_label": 0,
      "subject": "RE: server is rejecting mail from single domain",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 617,
      "actual_label": 0,
      "subject": "NBC11.com's Noon News Update",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 618,
      "actual_label": 0,
      "subject": "Library of Congress Classification Weekly List 31, 2008",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 619,
      "actual_label": 0,
      "subject": "Library of Congress Junior Fellows Unearth Treasures",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 620,
      "actual_label": 0,
      "subject": "Re: SASL questions",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 621,
      "actual_label": 1,
      "subject": "Full presentation Shakira",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 622,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 623,
      "actual_label": 1,
      "subject": "100% Safe To Take, With NO Side Effects.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 624,
      "actual_label": 1,
      "subject": "Relax. Take a Deep Breath We have the answers you seek.",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 625,
      "actual_label": 0,
      "subject": "Re: server is rejecting mail from single domain",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 626,
      "actual_label": 1,
      "subject": "Ravishing Bvlgari watches at Replica Classics   ",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 627,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 628,
      "actual_label": 0,
      "subject": "Re: sa-learn weirdness...",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 629,
      "actual_label": 1,
      "subject": "Drink from the fountain of youth",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 630,
      "actual_label": 0,
      "subject": "svn commit: r581581 - /spamassassin/rules/trunk/sandbox/jm/22_bug_5667.cf",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 631,
      "actual_label": 1,
      "subject": "Love consultants recommend herbal products",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 632,
      "actual_label": 1,
      "subject": "Befriend Jenna Jameson",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 633,
      "actual_label": 1,
      "subject": "Top Quality Size",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 634,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 635,
      "actual_label": 1,
      "subject": "InternationalPharmacyForCustomersCustomerSupport",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 636,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 637,
      "actual_label": 0,
      "subject": "[Bug 5813] [review] several TLDs are not parsed by URI text scanner in PerMsgStatus.pm",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 638,
      "actual_label": 1,
      "subject": "Revolutionary mens health aid has arrived",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 639,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 640,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 641,
      "actual_label": 0,
      "subject": "Re: server is rejecting mail from single domain",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 642,
      "actual_label": 1,
      "subject": "Perfected Pill Facilitation",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 643,
      "actual_label": 0,
      "subject": "[Bug 5780] [review] URI processing turns uuencoded strings into http URI's which then causes FPs",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 644,
      "actual_label": 1,
      "subject": "re: \u041a\u0410\u0427\u0415\u0421\u0422\u0412\u0415\u041d\u041d\u042b\u0415 \u041f\u041b\u0410\u0421\u0422\u0418\u041a\u041e\u0412\u042b\u0415 \u041e\u041a\u041d\u0410",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 645,
      "actual_label": 1,
      "subject": "Befriend Jenna Jameson",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 646,
      "actual_label": 0,
      "subject": "svn commit: r581582 - /spamassassin/trunk/rules/50_scores.cf",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 647,
      "actual_label": 0,
      "subject": "FW: So what is a 'Camel Toe' ?",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 648,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] squirrelmail v1.4.13 and Login Redirect Plugin v2.1",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 649,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] Autorespond plugin",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 650,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes storage.py, 1.63.2.1,\t1.63.2.2",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 651,
      "actual_label": 0,
      "subject": "Re: server is rejecting mail from single domain",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 652,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes/core_resources - New\tdirectory",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 653,
      "actual_label": 1,
      "subject": "Perfected RX Discounts",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 654,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 655,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CoreUI.py, 1.1.2.2, 1.1.2.3",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 656,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 657,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 658,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes setup.py,1.32.2.1,1.32.2.2",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 659,
      "actual_label": 1,
      "subject": "Stars foretell the best life likl rf omvu",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 660,
      "actual_label": 1,
      "subject": "Engaging Medical Offers",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 661,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] SM 1.4.13 Configuration Question: Mail Domain\tParameter",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 662,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 663,
      "actual_label": 1,
      "subject": "Karma sutra and massage foreplay techniques",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 664,
      "actual_label": 1,
      "subject": "Superior Medical Reductions",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 665,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 666,
      "actual_label": 0,
      "subject": "Article contribution letter for \"The Open Artificial Intelligence\tJournal\"",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 667,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 668,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 669,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 670,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 671,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 672,
      "actual_label": 0,
      "subject": "Re: server is rejecting mail from single domain",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 673,
      "actual_label": 1,
      "subject": "Keep the young girls away from that bulch",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 674,
      "actual_label": 1,
      "subject": "No Exam or Classes Require",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 675,
      "actual_label": 0,
      "subject": "Re: mass-check --server bug? ",
      "predicted_label": 0,
      "trust_score": 90,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 676,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/scripts core_server.py, 1.1.2.1,\t1.1.2.2",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 677,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CoreUI.py, 1.1.2.3,\t1.1.2.4 Options.py, 1.141.2.1, 1.141.2.2",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 678,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] errors",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 679,
      "actual_label": 0,
      "subject": "Re: Mail attachements truncated",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 680,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] errors",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 681,
      "actual_label": 0,
      "subject": "Re: perl5.005_05-MAINT20070902",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 682,
      "actual_label": 0,
      "subject": "Re: server is rejecting mail from single domain",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 683,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 684,
      "actual_label": 1,
      "subject": "Price for Viagra 100mg x 90 pills US $ 1.78 Per Pill",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 685,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes WebAppPlugin.py, NONE,\t1.1.2.1",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 686,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 687,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 688,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 689,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 690,
      "actual_label": 0,
      "subject": "[Bug 5667] add new URIBL lookup on rhsbl.ahbl.org",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 691,
      "actual_label": 1,
      "subject": "Womens wish list for men",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 692,
      "actual_label": 1,
      "subject": "Replica Rolex Watches ! SAVE Big !",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 693,
      "actual_label": 0,
      "subject": "[UAI] CFP: Constraint Solving & Programming at 20th Annual ACM Symposium on Applied Computing",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 694,
      "actual_label": 1,
      "subject": "Help Stop Premature Ejaculation! ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 695,
      "actual_label": 1,
      "subject": "WelcomeBestsellersVisaAccepted",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 696,
      "actual_label": 0,
      "subject": "2008 AAAI Fellows Nomination Deadline",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 697,
      "actual_label": 1,
      "subject": "Click here to purchase high-quality replica watch ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 698,
      "actual_label": 1,
      "subject": "You have received an eCard",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 699,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5745)",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 700,
      "actual_label": 1,
      "subject": "Order Rolex Replica //atches 0nline!   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 701,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 702,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 703,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 704,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 705,
      "actual_label": 0,
      "subject": "Is it possible to remove a hard drive partition? If yes, how?",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 706,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 707,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 708,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 709,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 710,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 711,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 712,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 713,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 714,
      "actual_label": 1,
      "subject": "Revolutionary mens health aid has arrived",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 715,
      "actual_label": 1,
      "subject": "Achieve any length you need",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 716,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes dnscache.py,1.3,1.3.2.1",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 717,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 718,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 719,
      "actual_label": 1,
      "subject": "Order this  anti-smoking patch for the one you love.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 720,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5746)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 721,
      "actual_label": 0,
      "subject": "[Bug 5815] New TLD .rs",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 722,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 723,
      "actual_label": 0,
      "subject": "buildbot failure in mleisi-suse10.2-x86_64",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 724,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 725,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 726,
      "actual_label": 0,
      "subject": "Re: Getting ? in spam scores.",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 727,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/scripts core_server.py, 1.1.2.2,\t1.1.2.3",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 728,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5747)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 729,
      "actual_label": 1,
      "subject": "Profits in 15 Minutes ?! MinuteProfits!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 730,
      "actual_label": 1,
      "subject": "Finest ange suggestion",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 731,
      "actual_label": 1,
      "subject": "Get a rod of colossal measurements!",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 732,
      "actual_label": 0,
      "subject": "ietf-dkim Digest, Vol 106, Issue 6",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 733,
      "actual_label": 1,
      "subject": "Love consultants recommend herbal products",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 734,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 735,
      "actual_label": 1,
      "subject": "Looking for a watch? Visit Replica Classics",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 736,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 737,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5748)",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 738,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 739,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 740,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 741,
      "actual_label": 1,
      "subject": "Payment confirmation #2711018404799517757",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 742,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 743,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes/core_resources ui.html,\t1.1.2.1, 1.1.2.2 ui_html.py, 1.1.2.1, 1.1.2.2",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 744,
      "actual_label": 1,
      "subject": "Items for man",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 745,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 746,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 747,
      "actual_label": 0,
      "subject": "[lwv-healthcare] Millions with Chronic illness go untreated - NY Times",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 748,
      "actual_label": 0,
      "subject": "buildbot failure in t-feisty-561",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 749,
      "actual_label": 0,
      "subject": "Britney Spears, the Rolling Stones, the 2008 Grammy Awards and more",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 750,
      "actual_label": 0,
      "subject": "[UAI] CP 2004 Call for Participation",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 751,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 752,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5749)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 753,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 754,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 755,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 756,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 757,
      "actual_label": 0,
      "subject": "ActiveState evaluation license for Komodo IDE",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 758,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5750)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 759,
      "actual_label": 1,
      "subject": "Which Ones Really Work? We List The Top Penis Enlargement Products!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 760,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 761,
      "actual_label": 1,
      "subject": "10 reasons to take enhancing medicaments.",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 762,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5751)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 763,
      "actual_label": 1,
      "subject": "Recommended plastic surgery for men",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 764,
      "actual_label": 1,
      "subject": "What They Don't Want You to Know What it Does to Your Body !",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 765,
      "actual_label": 1,
      "subject": "Womens wish list for men",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 766,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 767,
      "actual_label": 0,
      "subject": "ScienceNOW Daily Email Alert",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 768,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 769,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 770,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 771,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 772,
      "actual_label": 1,
      "subject": "Eat it",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 773,
      "actual_label": 1,
      "subject": "Penis Enlargement Is Possible ! Learn How and What Products Work",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 774,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 775,
      "actual_label": 0,
      "subject": "[Bug 5815] New TLD .rs",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 776,
      "actual_label": 1,
      "subject": "Sublime measurement can be achieved fast",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 777,
      "actual_label": 1,
      "subject": "Replica Classics  if looking for magnificent watches",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 778,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5752)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 779,
      "actual_label": 1,
      "subject": "Recommended plastic surgery for men",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 780,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 781,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 782,
      "actual_label": 0,
      "subject": "[Bug 5815] [review] New second-level domains for TLD .rs",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 783,
      "actual_label": 0,
      "subject": "Re: Getting ? in spam scores.",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 784,
      "actual_label": 0,
      "subject": "Re: [opensuse] 10.3 and powermanagement",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 785,
      "actual_label": 0,
      "subject": "[opensuse] Re: [opensuse-kde] KDE:KDE4 deleted",
      "predicted_label": 0,
      "trust_score": 90,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 786,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 787,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 788,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 789,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 790,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5753)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 791,
      "actual_label": 0,
      "subject": "CNET Download Dispatch: New Year's resolution redux",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 792,
      "actual_label": 0,
      "subject": "Re: [opensuse] Warning - 10.3/XGL repository has compiz/emerald version mismatch",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 793,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 794,
      "actual_label": 1,
      "subject": "More length and width",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 795,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 796,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 797,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 798,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 799,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 800,
      "actual_label": 1,
      "subject": "Big booty white girl rides black dick while being taped and exposed",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 801,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 802,
      "actual_label": 1,
      "subject": "For:9e1feefcb1ce395e87bae8f40782cff5 Paris and Britney nude.",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 803,
      "actual_label": 1,
      "subject": "Katerina age 29 -on dating ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 804,
      "actual_label": 1,
      "subject": "Ranking And Reviews Of The Top 5 Penis Enlargement Products!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 805,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CoreUI.py, 1.1.2.4,\t1.1.2.5 Options.py, 1.141.2.2, 1.141.2.3 UserInterface.py,\t1.61, 1.61.2.1 WebAppPlugin.py, 1.1.2.1, 1.1.2.2",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 806,
      "actual_label": 1,
      "subject": "Wanted Growth Enhancement testimonials",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 807,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 808,
      "actual_label": 0,
      "subject": "[spambayes-dev] spambayes crash due to bad image",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 809,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 810,
      "actual_label": 1,
      "subject": "Give your partner new feelings while have a sex dgjaa tlqmav d",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 811,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 812,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 813,
      "actual_label": 0,
      "subject": "[ydn-delicious] Re: Link checker for del.icio.us?",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 814,
      "actual_label": 0,
      "subject": "Re: [spambayes-dev] spambayes crash due to bad image",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 815,
      "actual_label": 0,
      "subject": "[Bug 5667] add new URIBL lookup on rhsbl.ahbl.org",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 816,
      "actual_label": 0,
      "subject": "Re: [opensuse] Intel 3945 Wireless Problem opensuse 10.3",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 817,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes ImageStripper.py, 1.14, 1.15",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 818,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 819,
      "actual_label": 0,
      "subject": "[ spambayes-Patches-1707808 ] imap-tte",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 820,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 821,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 822,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 823,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 824,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 825,
      "actual_label": 0,
      "subject": "Re: [opensuse] Intel 3945 Wireless Problem opensuse 10.3",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 826,
      "actual_label": 1,
      "subject": "You have an E-Card from...?",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 827,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 828,
      "actual_label": 1,
      "subject": "Proven Effective for 72 Hours.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 829,
      "actual_label": 1,
      "subject": "Great pharm shop",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 830,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 831,
      "actual_label": 1,
      "subject": "From Raymond Bliss",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 832,
      "actual_label": 0,
      "subject": "Re: [R] Greek characters in plots",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 833,
      "actual_label": 0,
      "subject": "[opensuse] openSuse 11 Alpha",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 834,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 835,
      "actual_label": 1,
      "subject": "More pleasure wit less efforts.",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 836,
      "actual_label": 1,
      "subject": "BestQualityPillsYourHealth",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 837,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 838,
      "actual_label": 1,
      "subject": "Omega replica watch Same quality at lower price   ",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 839,
      "actual_label": 0,
      "subject": "[ spambayes-Patches-1707808 ] imap-tte",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 840,
      "actual_label": 0,
      "subject": "Re: [opensuse] openSuse 11 Alpha",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 841,
      "actual_label": 0,
      "subject": "Re: perl5.005_05-MAINT20070902",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 842,
      "actual_label": 0,
      "subject": "Re: [opensuse] SUSE 10.3 + ATI FireGL 5250 + Latest ATI driver = no 3D efects",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 843,
      "actual_label": 0,
      "subject": "Re: [opensuse] ssh: Connection reset by peer",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 844,
      "actual_label": 0,
      "subject": "Re: [opensuse] A Quick Start - VirtualBox Howto",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 845,
      "actual_label": 0,
      "subject": "Re: [opensuse] Warning - 10.3/XGL repository has compiz/emerald version mismatch",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 846,
      "actual_label": 0,
      "subject": "buildbot failure in t-solaris-10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 847,
      "actual_label": 0,
      "subject": "Re: [opensuse] A Quick Start - VirtualBox Howto",
      "predicted_label": 0,
      "trust_score": 1,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 848,
      "actual_label": 1,
      "subject": "Affordable posh accessories",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 849,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 850,
      "actual_label": 0,
      "subject": "[UAI] New Journal: NMNC (fwd)",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 851,
      "actual_label": 0,
      "subject": "Ashwani Kaul, market analyst, Thomson Reuters, at 10:30 AM ET/ 7:30 AM PT on BNN",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 852,
      "actual_label": 1,
      "subject": "Excellent-made replica watches from Rolex   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 853,
      "actual_label": 1,
      "subject": "Give freedom to their dreams",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 854,
      "actual_label": 1,
      "subject": "For:42fcb2d48c638c1153ec87d1a563ac0f Boy Loses Arm in Gator Attack",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 855,
      "actual_label": 0,
      "subject": "Re: [patch@31780] fixes for cpanplus on VMS",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 856,
      "actual_label": 1,
      "subject": "Your order",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 857,
      "actual_label": 1,
      "subject": "Dont waste  time on hesitations, a guaranteed result!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 858,
      "actual_label": 0,
      "subject": "[ spambayes-Patches-1707808 ] imap-tte",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 859,
      "actual_label": 1,
      "subject": "AllProductsSoftTabsFDAapproved",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 860,
      "actual_label": 1,
      "subject": "How would You like to divert 1000s of fresh new visitors daily",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 861,
      "actual_label": 0,
      "subject": "41 new messages in 20 topics - abridged",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 862,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 863,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CorePlugin.py, NONE,\t1.1.2.1 XMLRPCPlugin.py, NONE, 1.1.2.1",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 864,
      "actual_label": 0,
      "subject": "buildbot failure in jm-feisty",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 865,
      "actual_label": 0,
      "subject": "Back to business",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 866,
      "actual_label": 0,
      "subject": "4 new messages in 4 topics - abridged",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 867,
      "actual_label": 0,
      "subject": "Re: [opensuse] SUSE 10.3 + ATI FireGL 5250 + Latest ATI driver = no 3D efects",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 868,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5754)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 869,
      "actual_label": 0,
      "subject": "Re: perl5.005_05-MAINT20070902",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 870,
      "actual_label": 0,
      "subject": "Re: [opensuse] openSuSe 10.3 hangs after kernel update",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 871,
      "actual_label": 1,
      "subject": "MoneybackPolicyInternetPricesInfo",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 872,
      "actual_label": 1,
      "subject": "Enjoy The Sex Life You Deserve",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 873,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CorePlugin.py, 1.1.2.1,\t1.1.2.2 Options.py, 1.141.2.3, 1.141.2.4 XMLRPCPlugin.py,\t1.1.2.1, 1.1.2.2",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 874,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 875,
      "actual_label": 1,
      "subject": "You are the  next to quit. ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 876,
      "actual_label": 0,
      "subject": "Suspend2-users Digest, Vol 35, Issue 28",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 877,
      "actual_label": 1,
      "subject": "Penis Enlargement Is Possible ! Learn How and What Products Work",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 878,
      "actual_label": 1,
      "subject": "Looking for Tag Heur replica? Visit Replica Classics   ",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 879,
      "actual_label": 0,
      "subject": "Re: Logical Names on VMS",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 880,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 881,
      "actual_label": 0,
      "subject": "[soaplite] Digest Number 1746",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 882,
      "actual_label": 1,
      "subject": "Items for the man",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 883,
      "actual_label": 1,
      "subject": "Payment confirmation #8856279318964985918",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 884,
      "actual_label": 1,
      "subject": "Items for men",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 885,
      "actual_label": 1,
      "subject": "Stuff for Man's",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 886,
      "actual_label": 1,
      "subject": "Help Stop Premature Ejaculation! ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 887,
      "actual_label": 0,
      "subject": "TuxOnIce-users Digest, Vol 36, Issue 11",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 888,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 889,
      "actual_label": 1,
      "subject": "More massive love luger",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 890,
      "actual_label": 1,
      "subject": "Replica Rolex Swiss Watches   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 891,
      "actual_label": 1,
      "subject": "Payment confirmation #1260113133871611088",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 892,
      "actual_label": 0,
      "subject": "Strange headers on mail from SA users list",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 893,
      "actual_label": 0,
      "subject": "Re: [opensuse] Xorg ignores my screen resolution settings.",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 894,
      "actual_label": 0,
      "subject": "Re: Strange headers on mail from SA users list",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 895,
      "actual_label": 0,
      "subject": "Re: cpanplus::backend.pm module_tree requires case preserving file systems.",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 896,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5755)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 897,
      "actual_label": 0,
      "subject": "[UAI] 2nd CFP: FLAIRS 2005--Special Track on Uncertain Reasoning",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 898,
      "actual_label": 0,
      "subject": "Re: sa-learn --ham ground rules",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 899,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5756)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 900,
      "actual_label": 0,
      "subject": "Re: [R] Greek characters in plots",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 901,
      "actual_label": 1,
      "subject": "Increase Your Penis Width (Girth) By upto 20%.",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 902,
      "actual_label": 1,
      "subject": "Viagra and Cialis for everyone!",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 903,
      "actual_label": 0,
      "subject": "Microsoft: Streaming Office 'infringes license' | CNET News.com Alert ",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 904,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 905,
      "actual_label": 0,
      "subject": "[ie-rant] Re: hp pavilion issues",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 906,
      "actual_label": 1,
      "subject": "Discover new saving possibilities with  CanadianPharmacy.",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 907,
      "actual_label": 0,
      "subject": "Fwd: Re: mal heca",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 908,
      "actual_label": 1,
      "subject": "Re:Finden Sie Ihr Programm ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 909,
      "actual_label": 1,
      "subject": "Unique men's health store",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 910,
      "actual_label": 0,
      "subject": "Re: perl5.005_05-MAINT20070902",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 911,
      "actual_label": 1,
      "subject": "Revolutionary mens health aid has arrived",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 912,
      "actual_label": 1,
      "subject": "Answers to erectile dysfunction",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 913,
      "actual_label": 0,
      "subject": "Re: [ie-rant] Re: hp pavilion issues",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 914,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 915,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 916,
      "actual_label": 0,
      "subject": "Re: [ie-rant] Re: hp pavilion issues",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 917,
      "actual_label": 1,
      "subject": "Guaranteed Erection Fast",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 918,
      "actual_label": 0,
      "subject": "Re: perl5.005_05-MAINT20070902",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 919,
      "actual_label": 1,
      "subject": "Jenna Jameson wants you as BFF",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 920,
      "actual_label": 0,
      "subject": "Re: [opensuse] OpenSuse 11",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 921,
      "actual_label": 0,
      "subject": "Please help your Parents Centre",
      "predicted_label": 0,
      "trust_score": 60,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 922,
      "actual_label": 0,
      "subject": "Re: perl5.005_05-MAINT20070902",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 923,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5757)",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 924,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes/core_resources ui_html.py,\t1.1.2.2, 1.1.2.3",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 925,
      "actual_label": 0,
      "subject": "Re: [Python-Dev] Adventures with x64, VS7 and VS8 on Windows",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 926,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] website faq.txt,1.93,1.93.2.1",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 927,
      "actual_label": 0,
      "subject": "Re: [opensuse] ssh: Connection reset by peer",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 928,
      "actual_label": 1,
      "subject": "Replica Classics trendy replica watches for you   ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 929,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 930,
      "actual_label": 1,
      "subject": "You can find here all medicines what you want",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 931,
      "actual_label": 0,
      "subject": "[clamav-virusdb] Update (daily: 5758)",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 932,
      "actual_label": 1,
      "subject": "orleans experiment brae imaginate wail",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 933,
      "actual_label": 0,
      "subject": "RE: [ie-rant] Re: hp pavilion issues",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 934,
      "actual_label": 1,
      "subject": "Replica Rolex Swiss Watches   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 935,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 936,
      "actual_label": 1,
      "subject": "replica Watches! rolex, patek philippe, vacheron ...   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 937,
      "actual_label": 0,
      "subject": "there goes the neighbourhood",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 938,
      "actual_label": 1,
      "subject": "You Can Enlarge Your Penis By 3\" Compare Top 5 Penis Enlargers",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 939,
      "actual_label": 1,
      "subject": "Replica Rolex Swiss Watches   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 940,
      "actual_label": 0,
      "subject": "Re: [Bulk] [opensuse] A Quick Start - VirtualBox Howto",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 941,
      "actual_label": 0,
      "subject": "Fwd: To Sam",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 942,
      "actual_label": 1,
      "subject": "Most Popular Watches of All Trademarks   ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 943,
      "actual_label": 1,
      "subject": "Feeling more enthusiasm",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 944,
      "actual_label": 1,
      "subject": "InfoPharmAvailable",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 945,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 946,
      "actual_label": 1,
      "subject": "Payment confirmation #7608845599212560918",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 947,
      "actual_label": 0,
      "subject": "Re: [opensuse] How can I write man pages? Linuxdoc/Docbook anyone? Report on manedit",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 948,
      "actual_label": 1,
      "subject": "Satisfy your partner every night",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 949,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 950,
      "actual_label": 1,
      "subject": "Exclusive stuff for mans",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 951,
      "actual_label": 0,
      "subject": "[SM-USERS] image_buttons",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 952,
      "actual_label": 0,
      "subject": "[opensuse] Unable to download list of online reprositories",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 953,
      "actual_label": 0,
      "subject": "Re: Strange headers on mail from SA users list",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 954,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 955,
      "actual_label": 0,
      "subject": "Re: [Bulk] [opensuse] A Quick Start - VirtualBox Howto",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 956,
      "actual_label": 0,
      "subject": "beginners Digest 9 Feb 2008 14:36:12 -0000 Issue 3382",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 957,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/spambayes CorePlugin.py, 1.1.2.2,\t1.1.2.3 CoreUI.py, 1.1.2.5, 1.1.2.6 XMLRPCPlugin.py, 1.1.2.2, 1.1.2.3",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 958,
      "actual_label": 0,
      "subject": "[Spambayes-checkins] spambayes/scripts core_server.py, 1.1.2.3,\t1.1.2.4",
      "predicted_label": 0,
      "trust_score": 95,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 959,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 960,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 961,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 962,
      "actual_label": 1,
      "subject": "is ROLEX under 199 $ good for you? ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 963,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 964,
      "actual_label": 1,
      "subject": "From Lora Foley",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 965,
      "actual_label": 0,
      "subject": "[opensuse] installation via IPv6",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 966,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 967,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 968,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 969,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 970,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 971,
      "actual_label": 1,
      "subject": "100% Safe To Take, With NO Side Effects.",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 972,
      "actual_label": 0,
      "subject": "Fwd: Fwd: FW: In Honor of Stupid People",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 973,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 974,
      "actual_label": 1,
      "subject": "Katerina age 29 -on dating ",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 975,
      "actual_label": 1,
      "subject": "Give your partner new feelings while have a sex",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 976,
      "actual_label": 1,
      "subject": "hello from karina",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 977,
      "actual_label": 0,
      "subject": "Re: [opensuse] installation via IPv6",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 978,
      "actual_label": 1,
      "subject": "Let other people  know about your successful quitting.",
      "predicted_label": 1,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 979,
      "actual_label": 1,
      "subject": "ThankYouWorldwideSoftTabs",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 980,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 981,
      "actual_label": 0,
      "subject": "[IIU] OT: The funny side of spam",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 982,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 983,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 984,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 985,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 986,
      "actual_label": 0,
      "subject": "Re: [SM-USERS] image_buttons - Solution",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 987,
      "actual_label": 1,
      "subject": "Proven Effective for 72 Hours.",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 988,
      "actual_label": 0,
      "subject": "Re: [opensuse] openSuse 11 Alpha",
      "predicted_label": 0,
      "trust_score": 1,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 989,
      "actual_label": 1,
      "subject": "jolly fiddlestick psychotherapist kenton capo",
      "predicted_label": 1,
      "trust_score": 0,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 990,
      "actual_label": 1,
      "subject": "Send him male enhancing products",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 991,
      "actual_label": 1,
      "subject": "Choice IWC replica watches at Replica Classics   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 992,
      "actual_label": 1,
      "subject": "repertory marque shedir tabula tabula",
      "predicted_label": 0,
      "trust_score": 20,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 993,
      "actual_label": 0,
      "subject": "[ spambayes-Bugs-988095 ] DEP / NX causes Outlook to crash",
      "predicted_label": 0,
      "trust_score": 90,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 994,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 995,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 0,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 996,
      "actual_label": 1,
      "subject": "CNN.com Daily Top 10",
      "predicted_label": 0,
      "trust_score": 80,
      "is_correct": false,
      "llm_dataset_files": "llm_datasets\\invalid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 997,
      "actual_label": 0,
      "subject": "Time sheet",
      "predicted_label": 0,
      "trust_score": 100,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 998,
      "actual_label": 1,
      "subject": "Re: \u0417\u0430\u0432\u043e\u0434\u0441\u043a\u0438\u0435 \u0446\u0435\u043d\u044b \u043d\u0430 \u041f\u041b\u0410\u0421\u0422\u0418\u041a\u041e\u0412\u042b\u0415 \u041e\u041a\u041d\u0410 -\u041f\u0412\u0425",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    },
    {
      "index": 999,
      "actual_label": 1,
      "subject": "replica Watches! rolex, patek philippe, vacheron ...   ",
      "predicted_label": 1,
      "trust_score": 20,
      "is_correct": true,
      "llm_dataset_files": "llm_datasets\\valid_training_data.jsonl",
      "llm_model": "llama3.2:3b"
    }
  ]
}